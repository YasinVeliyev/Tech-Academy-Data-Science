{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7afb1c5-1f80-404a-9751-424a5fee5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit,KFold,GroupKFold,StratifiedGroupKFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,StackingClassifier,BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92455ca7-96d2-49e0-835b-e3a37b62f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain=pd.read_csv(\"data/Train.txt\",sep=\"#|;\",engine=\"python\",header=None,index_col = False)\n",
    "dataTest=pd.read_csv(\"data/Test.txt\",sep=\"#|;\",engine=\"python\",header=None,index_col = False)\n",
    "dataCros=pd.read_csv(\"data/Cross_Validation.txt\",sep=\"#|;\",engine=\"python\",header=None,index_col = False)\n",
    "\n",
    "# data=shuffle(pd.concat([dataTrain,dataTest,dataCros]))\n",
    "# data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33693bcd-4ad2-4424-8ca1-1db3552bfb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "542    0\n",
       "543    0\n",
       "544    0\n",
       "545    0\n",
       "546    0\n",
       "Length: 547, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCros.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5129e1bf-2bee-4b7f-a5b4-e6baeaa05a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4cbdbbe7-e542-4a7b-9ec0-b3ede8f6836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X=data.drop(0,axis=1)\n",
    "X=scaler.fit_transform(X)\n",
    "Y=data[0]\n",
    "X_train,X_test,Y_train,Y_test,=train_test_split(X,Y,train_size=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "17b15b48-f160-43ec-895e-5aa720d82da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 20188 is out of bounds for axis 0 with size 18170",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [303], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train[\u001b[38;5;241m20188\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20188 is out of bounds for axis 0 with size 18170"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "20102b9e-5a56-41e5-8402-7e96c39e171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 27.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def find_best_model(learning_algorithm,X,Y,expected_accuracy=0.75,model_selection=None,model_selection_params={},params={}):\n",
    "    import pickle\n",
    "    while True:\n",
    "        train_size=np.round(np.random.random(500),2)\n",
    "        train_size = train_size[(train_size>0.5)&(train_size<0.95)]\n",
    "        for size in train_size:\n",
    "            X_train,X_val,y_train,y_val = train_test_split(X,Y,train_size=size)\n",
    "            \n",
    "            #adding default parametrs If params is not empty and model_selection is not None\n",
    "            if model_selection:\n",
    "                for (key,value) in params.items():\n",
    "                    model = learning_algorithm()\n",
    "                    l=set()\n",
    "                    if hasattr(value,\"__iter__\") and type(value)!=str:\n",
    "                        l=set(value)\n",
    "                    else:\n",
    "                        l.add(value)\n",
    "                    l.add(model.__dict__.get(key))\n",
    "                    params[key]=list(l)\n",
    "                selected_model = model_selection(model,params,scoring=\"accuracy\",**model_selection_params)\n",
    "                model = selected_model.fit(X_train,y_train)\n",
    "            else:\n",
    "                model = learning_algorithm(**params)\n",
    "                model.fit(X_train,y_train)\n",
    "            \n",
    "            y_val_predict=model.predict(X_val)\n",
    "            test_accuracy=accuracy_score(y_val_predict,y_val)\n",
    "\n",
    "            print(f\"Train Size: {size}, Model: {learning_algorithm.__name__}, Test Accuracy:{test_accuracy:.3}\")\n",
    "            if test_accuracy>=expected_accuracy:\n",
    "                with open(f\"{learning_algorithm.__name__.lower()}.pkl\",\"wb\") as file:\n",
    "                    pickle.dump(model,file)\n",
    "                return test_accuracy,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ad4001f7-8829-41b9-93fb-6b7e8a781433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 0.53, Model: DecisionTreeClassifier, Test Accuracy:0.742\n",
      "Train Size: 0.55, Model: DecisionTreeClassifier, Test Accuracy:0.736\n",
      "Train Size: 0.77, Model: DecisionTreeClassifier, Test Accuracy:0.77\n",
      "Train Size: 0.7, Model: DecisionTreeClassifier, Test Accuracy:0.758\n",
      "Train Size: 0.8, Model: DecisionTreeClassifier, Test Accuracy:0.776\n"
     ]
    }
   ],
   "source": [
    "accuracy,model=find_best_model(DecisionTreeClassifier,X_train,Y_train,expected_accuracy=0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c78f6406-0c52-4791-8c2c-a2c8eccd43a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9742574257425742"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=model.predict(X_test)\n",
    "accuracy_score(y_predict,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2ccbeca8-579f-4ac9-964c-7e29a433a9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 0.6, Model: LogisticRegression, Test Accuracy:0.919\n",
      "Train Size: 0.56, Model: LogisticRegression, Test Accuracy:0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "param_log =   {\n",
    "    'max_iter' : range(300,500,20)\n",
    "    }\n",
    "accuracy,model=find_best_model(LogisticRegression,X_train,Y_train,expected_accuracy=0.92,params={\"max_iter\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102dba71-7b54-4d10-ac7d-4e2d5cba072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A decision tree classifier.\n",
      "\n",
      "    Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      "\n",
      "    splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "        The strategy used to choose the split at each node. Supported\n",
      "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "        the best random split.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "            - If int, then consider `max_features` features at each split.\n",
      "            - If float, then `max_features` is a fraction and\n",
      "              `max(1, int(max_features * n_features_in_))` features are considered at\n",
      "              each split.\n",
      "            - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"log2\", then `max_features=log2(n_features)`.\n",
      "            - If None, then `max_features=n_features`.\n",
      "\n",
      "            .. deprecated:: 1.1\n",
      "                The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
      "                in 1.3.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the randomness of the estimator. The features are always\n",
      "        randomly permuted at each split, even if ``splitter`` is set to\n",
      "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "        select ``max_features`` at random at each split before finding the best\n",
      "        split among them. But the best found split may vary across different\n",
      "        runs, even if ``max_features=n_features``. That is the case, if the\n",
      "        improvement of the criterion is identical for several splits and one\n",
      "        split has to be selected at random. To obtain a deterministic behaviour\n",
      "        during fitting, ``random_state`` has to be fixed to an integer.\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    class_weight : dict, list of dict or \"balanced\", default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If None, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      "        The classes labels (single output problem),\n",
      "        or a list of arrays of class labels (multi-output problem).\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance [4]_.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    max_features_ : int\n",
      "        The inferred value of max_features.\n",
      "\n",
      "    n_classes_ : int or list of int\n",
      "        The number of classes (for single output problems),\n",
      "        or a list containing the number of classes for each\n",
      "        output (for multi-output problems).\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    tree_ : Tree instance\n",
      "        The underlying Tree object. Please refer to\n",
      "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "        for basic usage of these attributes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DecisionTreeRegressor : A decision tree regressor.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      "    function on the outputs of :meth:`predict_proba`. This means that in\n",
      "    case the highest predicted probabilities are tied, the classifier will\n",
      "    predict the tied class with the lowest index in :term:`classes_`.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "           Learning\", Springer, 2009.\n",
      "\n",
      "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> from sklearn.tree import DecisionTreeClassifier\n",
      "    >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "    >>> iris = load_iris()\n",
      "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "    ...                             # doctest: +SKIP\n",
      "    ...\n",
      "    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(DecisionTreeClassifier().__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69047d90-5bee-4118-b19e-1fa8dc7193e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cd34c-c017-473c-b792-6d9485413156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "70087837-4d77-406d-ac2b-728f392b98e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 0.94, Model: RandomForestClassifier, Test Accuracy:0.968\n"
     ]
    }
   ],
   "source": [
    "params={'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50],\n",
    " 'max_features': ['log2', 'sqrt'],\n",
    " 'min_samples_leaf': [2, 4],\n",
    " 'min_samples_split': [ 5, 10],\n",
    " 'n_estimators': [200, 400, 600],\n",
    " \"criterion\":[\"gini\", \"entropy\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "accuracy,model=find_best_model(RandomForestClassifier,X_train,Y_train,expected_accuracy=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa901f-e660-4c51-a213-a95f82502c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "dedc6377-3879-4caa-963d-8f42035f4ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18170"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6caf2ee3-ab9c-46e6-9b28-684c4b6dba61",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18485 is out of bounds for axis 0 with size 18170",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [297], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m Y_xgb\u001b[38;5;241m=\u001b[39mY_train[Y_train\u001b[38;5;241m.\u001b[39misin(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m))]\n\u001b[0;32m----> 2\u001b[0m X_xgb\u001b[38;5;241m=\u001b[39mX_train[Y_xgb\u001b[38;5;241m.\u001b[39mindex]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18485 is out of bounds for axis 0 with size 18170"
     ]
    }
   ],
   "source": [
    "Y_xgb=Y_train[Y_train.isin(range(10))]\n",
    "X_xgb=X_train[Y_xgb.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8519bc77-ed2e-4c5b-85c7-a46e8acd8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 0.83, Model: XGBClassifier, Test Accuracy:0.976\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\"subsample\":[0.5, 0.75, 1],\n",
    "              \"colsample_bytree\":[0.5, 0.75, 1],\n",
    "              \"max_depth\":[2, 6, 12],\n",
    "              \"min_child_weight\":[1,5,15],\n",
    "              \"learning_rate\":[0.3, 0.1, 0.03],\n",
    "              \"n_estimators\":[120,150,200]}\n",
    "\n",
    "D=data[data[0].isin(range(10))]\n",
    "scaler=StandardScaler()\n",
    "X=D.drop(0,axis=1)\n",
    "X=scaler.fit_transform(X)\n",
    "Y=D[0]\n",
    "X_xgb_train,X_xgb_test,Y_xgb_train,Y_xgb_test=train_test_split(X,Y,train_size=0.90)\n",
    "accuracy,model=find_best_model(XGBClassifier,X_xgb_train,Y_xgb_train,expected_accuracy=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3dc5872c-6bb2-44e2-9e5a-e549c52919ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830316742081447"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=model.predict(X_xgb_test)\n",
    "accuracy_score(predict,Y_xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e0250088-cd52-4673-b9e3-c4f3d59148f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830316742081447"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"xgbclassifier.pkl\",\"rb\") as file:\n",
    "    model = pickle.load(file)\n",
    "accuracy_score(model.predict(X_xgb_test),Y_xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b0518-e87b-4352-a3af-ce56e513ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict=m.predict(X_test)\n",
    "accuracy_score(Y_predict,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ec4350-3bd5-4f45-ae11-4ef6dd019f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'splitter': 'best',\n",
       " 'max_depth': None,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'random_state': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'class_weight': None,\n",
       " 'ccp_alpha': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a63751-f9cc-4b9f-92ea-5f05ef3706a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7037797-86bb-4671-81f4-3f74543cfa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bca67-9b47-4bed-974b-4243ee8204f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337d764-5638-4cbe-b1e0-2fa5701c854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d070bc-6de8-498a-9d4b-c02e3d345319",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier(**{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50cca4-5874-481c-a9e9-c39c23b151df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e6d86-d1f2-4ad6-965d-af4c9ff01744",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr([],\"__iter__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef449609-8fa6-401f-b1f8-a7574340707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.__dict__.get(\"estimator5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45660c4b-2076-4d62-9151-3bfb2f462aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier().__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a14ccc-e8af-43c6-86d4-efae20e8b09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f06e1f-0344-41a3-8e07-64b9a9c96ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9d208-4f0d-4265-bbe8-b6bf6dc9ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086740a2-2b75-4751-a228-9c9cf9f2ba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aa50f57-8158-45d9-ab08-ecf26f4ce17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=set([1,2,3,4])\n",
    "b=set([1,2,3,5,6,7])\n",
    "a.union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3060c17-317a-4c83-9745-0cbfff978e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5, 6, 7}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fd012-fa75-4047-bfaa-b085019d8d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa721db-bb5f-4596-b446-714bfdf92d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c18edbb-3196-468f-8383-6b6022cc0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7cbdc-e494-4574-998b-915cc94670e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b99ea9-58e9-4f7e-abcd-866ed2e49d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739ab7f9-0687-4d82-8db7-40c082a3ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3203a109-1eaf-426a-b16d-4ec45ded00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eac8481-4265-4b11-8131-6394acfca667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3440/738525883.py:1: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  bool(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c51d5a-553d-4bbd-9d34-2e3ef808129c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
